\documentclass[]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[paper = letterpaper,
            lmargin = 1.07in, tmargin = 1.2in,
            centering]{geometry}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{48,48,48}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.94,0.87,0.69}{{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.87,0.87,0.75}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.86,0.86,0.80}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.86,0.64,0.64}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.75,0.75,0.82}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.86,0.64,0.64}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.80,0.58,0.58}{{#1}}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.50,0.62,0.50}{{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.94,0.94,0.56}{{#1}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.81,0.69}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.94,0.94,0.56}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.80,0.80,0.80}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.76,0.75,0.62}{{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.80,0.80,0.80}{{#1}}}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}

\author{Joby John}
\date{April 25, 2016}
\title{OpenStreeMap Analysis Using
MongoDB \\
Los Angeles, California}

\begin{document}
\maketitle
\paragraph{Audit and Cleanup}\label{audit-and-cleanup}

The los-angeles\_California.osm file is \textbf{1.9GB} in size. The
initial task is to read the elements of the xml task and convert the
elements in to json elements and produce a json file as output. Large
files like the osm file for Los Angeles are more than 1 GB in size and
pose a memory problem while using the elementTree.iterparse() method. In
order to iteratively parse the file, it was important to clear each
element after it was processed using the \textbf{element.clear()}
function.

%\paragraph{Initial Analysis}\label{initial-analysis}

Using \emph{tags.py}, we find: \{`lower': 3069661, `lower\_colon':
3020030, `other': 197944, `problemchars': 9\} Among the problem tags,
most ``problems'' were the use of space or . (period),; (semi colon).

Using \emph{map\_parser.py} Parsing the osm file we find the different
kinds of tags in the file. \{`bounds': 1,\\ `member': 84031,\\ `nd':
9439779,\\ `node': 7946023,\\ `relation': 11230,\\ `root': 1,\\ `tag':
6287644,\\ `way': 940999\}

Using the
\texttt{python  \#pymongo  db.command("collstats",'losangeles1')} we
find:\\ \{u'avgObjSize': 259.30946115078103,\\ u'count': 8886382,\\
u'ns': u'cities.losangeles1',\\ u'size': 2304322928.0,\\ u'storageSize':
3210674176.0,\\ \}

Counting the number of users, we find that there are about 3222 unique
users (pipeline8) that created the data for Los Angeles. However, in
curating the address field it was noticed that many of the
\emph{addr:street} have only a name and are without a proper ending
identifying the \emph{type} of way. These typeless streets are
maintained as is. Some nodes and ways are even from another country
(Austria). This has to taken care of during further wrangling with
mongoDB.

Not only are there documents from other cities but also, one should be
careful \emph{not} to \$match using the city name as the Los Angeles
region consists of many smaller cities. The dataset is also corrupted by
data from other states (within the US). It now becomes important to
ensure that the data for Los Angeles reflects only local data. One way
to ensure this would be to check for \textbf{Latitude} and
\textbf{Longitude} coordinates.

The region of interest is found using the following tags, which are
found in the beginning of the file: \textbf{minlat}: 33.298,
\textbf{maxlon}: -116.724, \textbf{minlon}: -119.437, \textbf{maxlat}:
34.583

Only those documents (with `type':`node') that lie in the area
determined by the above coordinates are retained. This is addressed
programatically in python (\emph{data\_json.py}) as we convert to json.

However, only documents of type `node' have latitude and longitude
specified. So documents of type `way' could not be curated in this way.
To overcome the problem of `way' tags that do not lie in the LA region
and do not have logitude and latitude fields a new method is used. This
is described below in the MongoDB section.

The output json file so produced has a size of \textbf{2.1 GB}.

Some findings: There are many
`is\_in:city',`is\_in:state\_code',`is\_in:county',`is\_in:count\emph{r}y',
tags which are valid (i.e.~no problem characters). It is possible to use
these documents and introduce them to the address field. But this was
not undertaken in the current verstion. Right now, these records are
included as-is.

Also, importantly, none of the non ``tag'' tags have problem characters.

However, for the final conversion of the osm to json file, a naive
approach that used only \textbf{element.iterparse()} and
\textbf{element.clear()} proved to be insufficient. Therefore, a
separate function \textbf{get\_element()} as suggested here
(http://effbot.org/elementtree/iterparse.htm) was included to process
the osm file.

\subsubsection{MongoDB}\label{mongodb}

We list the number of cities with the following pipeline:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Count all the cities}
\NormalTok{pipeline1 = [\{}\StringTok{"$match"}\NormalTok{:\{}\StringTok{"address.city"}\NormalTok{:\{}\StringTok{"$exists"}\NormalTok{:}\OtherTok{True}\NormalTok{\}\}\},}
            \NormalTok{\{}\StringTok{"$group"}\NormalTok{:\{}\StringTok{'_id'}\NormalTok{:}\StringTok{'$address.city'}\NormalTok{,}\StringTok{"count"}\NormalTok{:\{}\StringTok{"$sum"}\NormalTok{:}\DecValTok{1}\NormalTok{\}\}\},}
            \NormalTok{\{}\StringTok{"$group"}\NormalTok{:\{}\StringTok{'_id'}\NormalTok{:}\OtherTok{None}\NormalTok{,}\StringTok{"count"}\NormalTok{:\{}\StringTok{"$sum"}\NormalTok{:}\DecValTok{1}\NormalTok{\}\}\}] }
\end{Highlighting}
\end{Shaded}

By restricting the nodes and ways to the LA region's longitude and
latitude range, the number of unique cities reduced from 362 to 361.
Even this is not completely accurate list of cities. For example, there
are spurious cities like ``Ventura County'' and ``Newport Coast''. These
can be eliminated by checking against a list of legitimate California
cities. Interestingly the LA OSM file had many more spurious cities
(705) just a few weeks ago (cities from other countries and states).
Re-downloading the file, the new version seems to be cleaner.

However, as mentioned previously, only ``node'' type documents have the
latitude and longitude fields. The `way' type documents do not have that
field and hence we cannot weed spurious ``way'' documents.

In order to overcome this problem, the following method is used. 1.
Create a list of all the cities with type `node' (These are cities that
legitimately lie in the LA region as defined by the min and max latitude
and longitude).\\2. Create a list of all the cities with type `way'.
This list contains both cities in the LA region and spurious cities as
we could not filter them during the json creation.\\3. Take the
\textbf{Set difference} of the above two lists to get an approximate
list of all the spurious ways.\\4. In the list of spurious lists so
obtained, there are a few legitimate cities (of ways without
corresponding nodes or just because of a formate difference or a typo).
These legitimate ones can easily be weeded out manually to create a more
accurate list of spurious entries. Let's call this list Way\_NotLA
(i.e.~ways not in LA).\\5. Now, using the \textbf{\$nin} operator we can
easily avoid the cities that are not in the LA region even for the `way'
nodes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sort cities with max entries}
\NormalTok{pipeline5 = [\{}\StringTok{"$match"}\NormalTok{:\{}\StringTok{"address.city"}\NormalTok{:\{}\StringTok{"$exists"}\NormalTok{:}\OtherTok{True}\NormalTok{,}\StringTok{'$nin'}\NormalTok{:Way_NotLA\}\}\},}
            \NormalTok{\{}\StringTok{"$group"}\NormalTok{:\{}\StringTok{'_id'}\NormalTok{:}\StringTok{'$address.city'}\NormalTok{,}\StringTok{"count"}\NormalTok{:\{}\StringTok{"$sum"}\NormalTok{:}\DecValTok{1}\NormalTok{\}\}\},}
            \NormalTok{\{}\StringTok{"$sort"}\NormalTok{:\{}\StringTok{'count'}\NormalTok{:-}\DecValTok{1}\NormalTok{\}\}] }
\end{Highlighting}
\end{Shaded}

Now we find that there are \emph{332} unique cities: Of course there are
typos and a few reundancies like `Los Angeles',`Los \textbar{}Angeles',
and `los angeles, ca', Los Angeles-Venice, etc. Even so, this is a more
representative filtering of the documents.

It is found by sorting the documents grouped by `address.city' that the
city \emph{`Irvine'} has the most documents (24304).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Zipcode with most entries}
\NormalTok{pipeline6 = [\{}\StringTok{"$match"}\NormalTok{:\{}\StringTok{"address.postcode"}\NormalTok{:\{}\StringTok{"$exists"}\NormalTok{:}\OtherTok{True}\NormalTok{\}\}\},}
            \NormalTok{\{}\StringTok{"$group"}\NormalTok{:\{}\StringTok{'_id'}\NormalTok{:}\StringTok{'$address.postcode'}\NormalTok{,}\StringTok{"count"}\NormalTok{:\{}\StringTok{"$sum"}\NormalTok{:}\DecValTok{1}\NormalTok{\}\}\},}
            \NormalTok{\{}\StringTok{"$sort"}\NormalTok{:\{}\StringTok{'count'}\NormalTok{:-}\DecValTok{1}\NormalTok{\}\},}
            \NormalTok{\{}\StringTok{"$limit"}\NormalTok{:}\DecValTok{10}\NormalTok{\}] }\CommentTok{# sort cities with max entries       }
\end{Highlighting}
\end{Shaded}

The zipcode 92630 is found to have most mentions (15097 docs). The other
top zipcodes are 92028 (Fallbrook), 92620(Irvine), 92860 (Norco),
92618(Irvine), 92602 (Irvine), 92672 (San Clemente).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#City with the most unique zipcodes}
\NormalTok{pipeline7 = [\{}\StringTok{"$match"}\NormalTok{:\{}\StringTok{"address.postcode"}\NormalTok{:\{}\StringTok{"$exists"}\NormalTok{:}\OtherTok{True}\NormalTok{,}
            \StringTok{"address.city"}\NormalTok{:\{}\StringTok{"$exists"}\NormalTok{:}\OtherTok{True}\NormalTok{,}\StringTok{'$nin'}\NormalTok{:Way_NotLA\},}
            \CommentTok{"address.city"}\NormalTok{:\{}\StringTok{"$exists"}\NormalTok{:}\OtherTok{True}\NormalTok{\}\}\},}
            \NormalTok{\{}\StringTok{"$group"}\NormalTok{:\{}\StringTok{'_id'}\NormalTok{:}\StringTok{'$address.city'}\NormalTok{,}
            \CommentTok{"postal_code"}\NormalTok{:\{}\StringTok{'$addToSet'}\NormalTok{:}\StringTok{'$address.postcode'}\NormalTok{\}\}\},}
            \NormalTok{\{}\StringTok{'$unwind'}\NormalTok{:}\StringTok{"$postal_code"}\NormalTok{\},}
            \NormalTok{\{}\StringTok{'$group'}\NormalTok{:\{}\StringTok{"_id"}\NormalTok{:}\StringTok{'$_id'}\NormalTok{,}\StringTok{'count'}\NormalTok{:\{}\StringTok{'$sum'}\NormalTok{:}\DecValTok{1}\NormalTok{\}\}\},            }
            \NormalTok{\{}\StringTok{'$sort'}\NormalTok{:\{}\StringTok{'count'}\NormalTok{:-}\DecValTok{1}\NormalTok{\}\},}
            \NormalTok{\{}\StringTok{"$limit"}\NormalTok{:}\DecValTok{10}\NormalTok{\}            }
            \NormalTok{]}
\end{Highlighting}
\end{Shaded}

By using the \emph{\$unwind} operator we can then \emph{\$addToSet} and
find the number of unique postal codes for a give city. The city of Los
Angeles come out at the top with 104 postal codes. Irvine (18), Long
Beach (13), Riverside (12), Pasadena (11) are the other cities with most
zipcode (top 5).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# User with most entry:}
\NormalTok{pipeline8 = [\{}\StringTok{"$group"}\NormalTok{:\{}\StringTok{'_id'}\NormalTok{:}\StringTok{'$created.user'}\NormalTok{,}\StringTok{'count'}\NormalTok{:\{}\StringTok{'$sum'}\NormalTok{:}\DecValTok{1}\NormalTok{\}\}\},}
            \NormalTok{\{}\StringTok{'$sort'}\NormalTok{:\{}\StringTok{'count'}\NormalTok{:-}\DecValTok{1}\NormalTok{\}\},}
            \NormalTok{\{}\StringTok{"$limit"}\NormalTok{:}\DecValTok{10}\NormalTok{\}            }
            \NormalTok{] }\CommentTok{# The user with most entries}
\end{Highlighting}
\end{Shaded}

We find that in the sheer number of documents created,
`woodpeck\_fixbot' leads the way, closely followed by Temecula\_Mapper.
AM909, kingrollo\_imports,N76\_import, nmixterare the other top
contributors with around or more than quarter million documents each.

However if we restrict attention to cities with city names or even
postcodes (i.e.~look at users who have contributed to most cities or
most postal codes), woodpeck\_fixbot is not in the top 50 even.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Users with contribution in most postal code.}
\NormalTok{pipeline10 = [\{}\StringTok{'$match'}\NormalTok{:\{}\StringTok{"address.postcode"}\NormalTok{:\{}\StringTok{'$exists'}\NormalTok{:}\OtherTok{True}\NormalTok{\}\}\},}
            \NormalTok{\{}\StringTok{"$group"}\NormalTok{:\{}\StringTok{'_id'}\NormalTok{:}\StringTok{'$created.user'}\NormalTok{,}
            \CommentTok{"postal_code"}\NormalTok{:\{}\StringTok{'$addToSet'}\NormalTok{:}\StringTok{'$address.postcode'}\NormalTok{\}\}\},}
            \NormalTok{\{}\StringTok{'$unwind'}\NormalTok{:}\StringTok{'$postal_code'}\NormalTok{\},}
            \NormalTok{\{}\StringTok{'$group'}\NormalTok{:\{}\StringTok{'_id'}\NormalTok{:}\StringTok{'$_id'}\NormalTok{,}\StringTok{'count'}\NormalTok{:\{}\StringTok{'$sum'}\NormalTok{:}\DecValTok{1}\NormalTok{\}\}\},}
            \NormalTok{\{}\StringTok{'$sort'}\NormalTok{:\{}\StringTok{'count'}\NormalTok{:-}\DecValTok{1}\NormalTok{\}\},}
            \NormalTok{\{}\StringTok{"$limit"}\NormalTok{:}\DecValTok{90}\NormalTok{\}            }
            \NormalTok{]            }
\end{Highlighting}
\end{Shaded}

According to the above query pipeline: AM909, Brian@Brea,
palewire,nmixter, mmaxerickson are the top contributors by postal codes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Return all the restaurants}
\NormalTok{pipeline12 = [\{}\StringTok{'$match'}\NormalTok{:\{}\StringTok{"amenity"}\NormalTok{:\{}\StringTok{'$exists'}\NormalTok{:}\OtherTok{True}\NormalTok{,}\StringTok{'$in'}\NormalTok{:[}\StringTok{'restaurant'}\NormalTok{,}\StringTok{'Restaurant'}\NormalTok{]\}\}\}]}
\end{Highlighting}
\end{Shaded}

Number of restaurants in the LA region is \emph{3131}.

\paragraph{Restaurants Near A
Location:}\label{restaurants-near-a-location}

One can use geoIndexing to query the number of amenities in a region:
for example, a very common query would be to find all the restaurants
within x miles of a given location, e.g.~Staple's Center.

\emph{Problems} encountered in this approach: See pipeline13 and
pipeline14:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pipeline13 =   [\{}\StringTok{'$geoNear'}\NormalTok{:\{}\StringTok{'near'}\NormalTok{:[ -}\FloatTok{118.26684}\NormalTok{, }\FloatTok{34.04302} \NormalTok{],}\StringTok{'spherical'}\NormalTok{:}\OtherTok{True}\NormalTok{,}
\CommentTok{'maxDistance'}\NormalTok{:}\DecValTok{10}\NormalTok{/}\FloatTok{3963.2}\NormalTok{,}\StringTok{'distanceField'}\NormalTok{:}\StringTok{'distance'}\NormalTok{,}\StringTok{'includeLocs'}\NormalTok{:}\StringTok{'pos'}\NormalTok{,}
\CommentTok{'distanceMultiplier'}\NormalTok{:}\FloatTok{3963.2}\NormalTok{,}\StringTok{'spherical'}\NormalTok{:}\OtherTok{True}\NormalTok{,}\StringTok{'limit'}\NormalTok{:}\DecValTok{200000}\NormalTok{,}\StringTok{'query'}\NormalTok{:pipeline12\}\}]}
\end{Highlighting}
\end{Shaded}

In the above query 3963.2 is tha radius of the earth and is used to
convert `maxDistance' to radians
($d=r\theta \: \Rightarrow \: \theta=d/r$)\\The last item in the query
specifies pipeline12 which is a query of documents with
amenity==restaurant. However, pipeline13 returns documents that do not
meet this constraint. It seems that \emph{query=pipeline13 does nothing}
It returns $2e5$ documents in the vicinity of Staple's Center. We then
use another query to extend this pipeline to return only restaurants.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{pipeline14 = [\{}\StringTok{'$geoNear'}\NormalTok{:\{}\StringTok{'near'}\NormalTok{:[ -}\FloatTok{118.26684}\NormalTok{, }\FloatTok{34.04302} \NormalTok{],}
                           \StringTok{'spherical'}\NormalTok{:}\OtherTok{True}\NormalTok{,}\StringTok{'maxDistance'}\NormalTok{:}\DecValTok{10}\NormalTok{/}\FloatTok{3963.2}\NormalTok{,}
                          \CommentTok{'distanceField'}\NormalTok{:}\StringTok{'distance'}\NormalTok{,}\StringTok{'includeLocs'}\NormalTok{:}\StringTok{'pos'}\NormalTok{,}
                          \CommentTok{'distanceMultiplier'}\NormalTok{:}\FloatTok{3963.2}\NormalTok{,}\StringTok{'limit'}\NormalTok{:}\DecValTok{50000}\NormalTok{\}\},                              }
          \NormalTok{\{}\StringTok{'$match'}\NormalTok{:\{}\StringTok{"amenity"}\NormalTok{:\{}\StringTok{'$exists'}\NormalTok{:}\OtherTok{True}\NormalTok{,$in}\StringTok{':['}\NormalTok{restaurant}\StringTok{','}\NormalTok{Restaurant}\StringTok{']}\OtherTok{\}\}}\StringTok{\},}
\StringTok{          \{'}\NormalTok{$project}\StringTok{':\{'}\NormalTok{_id}\StringTok{':'}\NormalTok{$_id}\StringTok{',"amenity":"$amenity",}
\StringTok{                           '}\NormalTok{name}\StringTok{':'}\NormalTok{$name}\StringTok{','}\NormalTok{dist}\StringTok{':"$distance",'}\NormalTok{pos}\StringTok{':'}\NormalTok{$pos}\StringTok{'}\OtherTok{\}\}}\StringTok{]}
\StringTok{    }
\end{Highlighting}
\end{Shaded}

The above pipeline has a flaw that the limit is applied to all the
results and the resulting collection is then queried with a \$match. If
we are near a high density point (like Staple's Center in downtown LA)
then the \$geoNear query might have more than 50000 results and
therefore we might not be capturing all the valid restaurants.

To overcome this, a plain db.command() is used as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result =db.command(}\StringTok{'geoNear'}\NormalTok{,}\StringTok{'losangeles1'}\NormalTok{,  }
                   \NormalTok{near=\{}\StringTok{'type'}\NormalTok{:}\StringTok{'Point'}\NormalTok{,  }
                         \CommentTok{'coordinates'}\NormalTok{:[-}\FloatTok{118.26684}\NormalTok{, }\FloatTok{34.04302}\NormalTok{]\},  }
                 \NormalTok{spherical=}\OtherTok{True}\NormalTok{,  }
                 \NormalTok{maxDistance=}\DecValTok{333}\NormalTok{, }\CommentTok{# Around 1000 feet.   }
                 \NormalTok{distanceField=}\StringTok{'distance'}\NormalTok{,  }
                 \NormalTok{includeLocs=}\StringTok{'pos'}\NormalTok{,  }
                 \NormalTok{query=\{}\StringTok{"amenity"}\NormalTok{:\{}\StringTok{'$in'}\NormalTok{:[}\StringTok{'restaurant'}\NormalTok{]\}\})  }
\end{Highlighting}
\end{Shaded}

We find there are only four restaurants within 1000 feet
(\textasciitilde{}330 m) of the Staple's Center.By adding the following
to the query

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{query=\{}\StringTok{"amenity"}\NormalTok{:\{}\StringTok{'$in'}\NormalTok{:[}\StringTok{'restaurant'}\NormalTok{]\},}\StringTok{"cuisine"}\NormalTok{:\{}\StringTok{'$regex'}\NormalTok{:}\StringTok{'.vegan.'}\NormalTok{\}\}}
\end{Highlighting}
\end{Shaded}

we find that there is only a single restaurant (Souplantation) that
offers vegan options in the LA region. Another query on the name reveals
that there are 3 Souplantations in the region.

\paragraph{Other ideas For Dataset:}\label{other-ideas-for-dataset}

The audit process on the streetnames makes sure that the street names
follow a certain convention. A similar approach can be applied to clean
up the city names. Right now city names were partially curated by
checking against the list of city names on nodes in the LA region. We
can take a more aggressive approach by making sure that the city names
do not have the state's name along with it (`CA', `ca') by checking
pruning the last characters.

A similar check can be applied for postal codes to follow the 5 digit
pattern. Also, they can also be checked against a list of valid postal
codes that belong to the LA region. This list of valid zipcodes could
also be generated from the nodes whos coordinates lie within the
latitude and longitude of the region.

 Using k-means clustering:\\K-means clustering can be used to identiy
clusters of nodes that people are interested in, in a given city. By
selecting say 20-30 different clusters for a region like LA, we can
classify the nodes/ways in to different clusters. By idenitifying the
clusters with most points, one can discover trends of ``interesting
areas'' and guide users to regions with less points to increase use
contribution in those areas. One would plot a color-coded cluster map of
nodes on a map of LA to visualize the clustering of points. The color
code would correspong to cluster number that each node belongs to.
Psuedo-code for this is presented in \emph{psuedo\_code\_kMeans.py}.

\center{ \noindent\rule{8cm}{0.4pt}}

\subsection*{File Names}
\begin{itemize}
\item \emph{tags.py}
Counts all the tags in the OSM file.

\item \emph{Users.py}
Counts the number of unique users (3222) in the OSM file and returns their UIDs.
 
\item \emph{Audit.py}
Counts the number of unique street types in the OSM file.
Also counts the number of city names in OSM file.


\item \emph{data\_json.py}
Converts the OSM file to json.
For tags of type "node" with a latitude and longitude, determines if it belongs to the LA region or not.
Weeds out the documents with addr:city not in the LA region.
Relies on curate\_city\_way\_node.py to get the list Way\_NotLA (which is the list of cities not in LA.)

\item \emph{city\_way\_node.py}
This documents collects the result of two different pipelines of mongoDB query 
and takes the difference of the cities in "nodes" and cities in "ways" to come up with the cities not in LA region and stores it in the list Way\_NotLA

\item \emph{Mongo\_aggregate1.py}
The main script that runs different queries on the collection 'losangeles1' in the database 'cities'
These pipelines have been discussed in the pdf document.

\item \emph{psuedo\_code\_kMeans.py}
Pythonish psuedocode describing how one could find clustering within the map data.

\item \emph{resize\_OSM.py}
Script to downsize the osm to a smaller size.
\end{itemize}
\end{document}
