\documentclass[12pt]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage[paper = letterpaper,
            lmargin = 1.07in, tmargin = 1.5in,
            centering]{geometry}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}

\date{}

\begin{document}

\subsubsection{OpenStreeMap Analysis Using
MongoDB:}\label{openstreemap-analysis-using-mongodb}

\paragraph{Los Angeles, California}\label{los-angeles-california}

\paragraph{Audit and Cleanup}\label{audit-and-cleanup}

Large files like the osm file for Los Angeles are more than 1 GB in size
and pose a memory problem while using the elementTree.iterparse()
method. In order to iteratively parse the file, it was important to
clear each element after it was processed using the
\textbf{element.clear()} function.

Initial Analysis of tags using elementTree iterparse. We see that the
file has,

Using \emph{tags.py}, we find: 2.77 million lower case tags\\2.72
million lower tags with colon\\36 tags with problem char\\and 210746
``other'' tags.

Using \emph{map\_parser.py} Parsing the osm file we find the different
kinds of tags in the file.
\{`bounds':1,\\`member':118031,\\`nd':8105933,\\`node':6310901,\\`relation':25076,\\`root':1,\\`tag':5708542,\\`way':903836\}

Among the problem tags, most ``problems'' were the use of space or
`.',`;',`/',`='

Counting the number of users, we find that there are about 9154 uniques
users that created the data for Los Angeles. However, in curating the
address field it was noticed that many of the \emph{addr:street} have
only a name and are without a proper ending identifying the \emph{type}
of way. These typeless streets are maintained as is. Some nodes and ways
are even from another country (Austria). This has to taken care of
during further wrangling with mongoDB.

Not only are there documents from other cities but also, one should be
careful \emph{not} to \$match using the city name as the Los Angeles
region consists of many smaller cities. The dataset is also corrupted by
data from other states (within the US). It now becomes important to
ensure that the data for Los Angeles reflects only local data. One way
to ensure this would be to check for \textbf{Latitude} and
\textbf{Longitude} coordinates.

The region of interest is found using the following tags, which are
found in the beginning of the file: \textbf{minlat}:
33.298\\\textbf{maxlon}: -116.724\\\textbf{minlon}:
-119.437\\\textbf{maxlat}: 34.583

Only those documents (with `type':`node') that lie in the area
determined by the above coordinates are retained. This is addressed
programatically in python (\emph{data\_json.py}) before we convert to
json.

However, only documents of type `node' have latitude and longitude
specified. So documents of type `way' could not be curated in this way.

To overcome the problem of `way' tags that do not lie in the LA region
and do not have logitude and latitude fields a new method is used. This
is described below in the MongoDB section.

Some findings: There are many
`is\_in:city',`is\_in:state\_code',`is\_in:county',`is\_in:count\emph{r}y',
tags which are valid (i.e.~no problem characters). It is possible to use
these documents and introduce them to the address field. But this was
not undertaken in the current verstion. Right now, these records are not
processed.

Also, importantly, none of the non ``tag'' tags have problem characters.

However, for the final conversion of the osm to json file, a naive
approach that used only \textbf{element.iterparse()} and
\textbf{element.clear()} proved to be insufficient. Therefore, a
separate function \textbf{get\_element()} as suggested here
(http://effbot.org/elementtree/iterparse.htm) was included to process
the osm file.

\subsubsection{MongoDB}\label{mongodb}

We list the number of cities with the following pipeline:

\begin{verbatim}
# Count all the cities
pipeline1 = [{"$match":{"address.city":{"$exists":True}}},
            {"$group":{'_id':'$address.city',"count":{"$sum":1}}},
            {"$group":{'_id':None,"count":{"$sum":1}}}] 
\end{verbatim}

By restricting the nodes and ways to the LA region's longitude and
latitude range, the number of unique cities reduced from 705 to 289.
Even this is not completely accurate list of cities. For example, there
are spurious cities like ``Ventura County'' and ``Newport Coast''. These
can be eliminated by checking against a list of legitimate California
cities.

However, as mentioned previously, only ``node'' type documents have the
latitude and longitude fields. The `way' type documents do not have that
field and hence we cannot weed spurious ``way'' documents.

In order to overcome this problem, the following method is used. 1.
Create a list of all the cities with type `node' (These are cities that
legitimately lie in the LA region as defined by the min and max latitude
and longitude). 2. Create a list of all the cities with type `way'. This
list contains both cities in the LA region and spurious cities as we
could not filter them during the json creation 3. Take the \textbf{Set
difference} of the above two lists to get an approximate list of all the
spurious ways. 3.5 In the list of spurious lists so obtained, there are
a few legitimate cities (of ways without corresponding nodes or just
because of a formate difference or a typo). These legitimate ones can
easily be weeded out manually to create a more accurate list of spurious
entries. Let's call this list Way\_NotLA (i.e.~ways not in LA). 4. Now,
using the \textbf{\$nin} operator we can easily avoid the cities that
are not in the LA region even for the `way' nodes.

\begin{verbatim}
# sort cities with max entries
pipeline2 = [{"$match":{"address.city":{"$exists":True,'$nin':Way_NotLA}}},
            {"$group":{'_id':'$address.city',"count":{"$sum":1}}},
            {"$sort":{'count':-1}}] 
\end{verbatim}

Now we find that there are \emph{334} unique cities: Of course there are
typos and a few reundancies like `Los Angeles',`Los \textbar{}Angeles',
and `los angeles, ca'. Even so, this is a more representative filtering
of the documents.

\begin{verbatim}
# sort cities with max entries
pipeline2 = [{"$match":{"address.city":{"$exists":True,'$nin':Way_NotLA}}},
            {"$group":{'_id':'$address.city',"count":{"$sum":1}}},
            {"$sort":{'count':-1}}] 
\end{verbatim}

It is found by sorting the documents grouped by `address.city' that the
city \emph{`Irvine'} has the most documents (24304).

\begin{verbatim}
# Zipcode with most entries
pipeline3 = [{"$match":{"address.postcode":{"$exists":True}}},
            {"$group":{'_id':'$address.postcode',"count":{"$sum":1}}},
            {"$sort":{'count':-1}},
            {"$limit":10}] # sort cities with max entries       
\end{verbatim}

The zipcode 92630 is found to have most mentions (15096 docs). The other
top zipcodes are 92028 (Fallbrook), 92620(Irvine), 92860 (Norco),
92618(Irvine), 92602 (Irvine), 92672 (San Clemente).

\begin{verbatim}
#City with the most unique zipcodes
pipeline4 = [{"$match":{"address.postcode":{"$exists":True,"address.city":{"$exists":True,'$nin':Way_NotLA},
            "address.city":{"$exists":True}}},
            {"$group":{'_id':'$address.city',
            "postal_code":{'$addToSet':'$address.postcode'}}},
            {'$unwind':"$postal_code"},
            {'$group':{"_id":'$_id','count':{'$sum':1}}},            
            {'$sort':{'count':-1}},
            {"$limit":10}            
            ]
\end{verbatim}

By using the \emph{\$unwind} operator we can then \emph{\$addToSet} and
find the number of unique postal codes for a give city. The city of Los
Angeles come out at the top with 102 postal codes. Irvine (18), Long
Beach (13), Riverside (12), Pasadena (11) are the other cities with most
zipcode (top 5).

\begin{verbatim}
# User with most entry:
pipeline5 = [{"$group":{'_id':'$created.user','count':{'$sum':1}}},
            {'$sort':{'count':-1}},
            {"$limit":10}            
            ] # The user with most entries
\end{verbatim}

We find that in the sheer number of documents created,
`woodpeck\_fixbot' leads the way, closely followed by
``Temecula\_Mapper''. AM909, nmixter, SJfriedl, Biran@Brea are the other
top contributors with around or more than quarter million docs.

However if we restrict attention to cities with city names or even
postcodes, woodpeck\_fixbot is not in the top 50 even.

\begin{verbatim}
# Users with contribution in most postal code.
pipeline7 = [{'$match':{"address.postcode":{'$exists':True}}},
            {"$group":{'_id':'$created.user',
            "postal_code":{'$addToSet':'$address.postcode'}}},
            {'$unwind':'$postal_code'},
            {'$group':{'_id':'$_id','count':{'$sum':1}}},
            {'$sort':{'count':-1}},
            {"$limit":90}            
            ]            
\end{verbatim}

According to the above query pipeline: AM909, Brian@Brea,
palewire,nmixtermmaxerickson are the top contributors by postal codes.

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\end{document}
